{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2myMnSaNgnO"
   },
   "source": [
    "# Loading Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IG9VvcZgNgLi"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Preprocessing\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "\n",
    "# FastText\n",
    "import fasttext\n",
    "import fasttext.util\n",
    "\n",
    "import random\n",
    "\n",
    "# MEN and SimLex Benchmarks\n",
    "from os import listdir\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ElasticNet and ANN\n",
    "import sklearn\n",
    "from sklearn.model_selection import cross_val_score, RepeatedKFold, train_test_split, KFold, LeaveOneOut, StratifiedKFold\n",
    "from sklearn.metrics import mean_absolute_error, accuracy_score, mean_squared_error, median_absolute_error\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "from numpy.random import seed\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import time\n",
    "\n",
    "from statistics import median\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.random import set_seed\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from tensorflow.keras.layers import MaxPooling2D, MaxPooling1D, Conv2D, Conv1D, Bidirectional\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "siNP6eCLNWtS"
   },
   "outputs": [],
   "source": [
    "path = \"drive/My Drive/Thesis/Data/CoCA/Text/\"                                  ## These are the paths to easily export/import my dicts, txts, models, and pickles\n",
    "dict_path = \"drive/My Drive/Thesis/Data/CoCA/dict_pickles/\"\n",
    "unclean_path = path + \"texts_combined/all_texts_combined.txt\"\n",
    "model_path = \"drive/My Drive/Thesis/Data/CoCA/models/\"\n",
    "pickle_path = \"drive/MyDrive/Thesis/Data/fastText and others/\"\n",
    "norms_path = \"drive/My Drive/Thesis/Data/Norms/\"\n",
    "csv_path = \"drive/My Drive/Thesis/Data/CSV/\"\n",
    "delta_path = \"drive/My Drive/Thesis/Data/Deltacode/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kr14yLUFNZJl"
   },
   "source": [
    "# Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KtgAJ1q4NYPs"
   },
   "outputs": [],
   "source": [
    "### Read CSV File and Delete Unimportant Columns (i.e., everything that isn't the name, name type, rating, or the author's choice)\n",
    "\n",
    "### This is input for the FT model, which itself is the input for the ElasticNet and ANN regressions\n",
    "\n",
    "names_ratings = read_csv(\"drive/MyDrive/Thesis/Data/giovanni_email_data/avgRatings_annotated.csv\")\n",
    "\n",
    "#print(names_ratings.head())\n",
    "\n",
    "print(names_ratings['rating.mean_age'].notna().sum())                           ## Choosing only those rows where all columns are not NA\n",
    "print(names_ratings['rating.mean_gender'].notna().sum())\n",
    "print(names_ratings['rating.mean_valence'].notna().sum())\n",
    "\n",
    "df_age = names_ratings.loc[names_ratings['rating.mean_age'].notna(), ['name', 'rating.mean_age', 'age', 'name_type']]   ## Choosing the relevant columns\n",
    "print(df_age.head(), len(df_age))\n",
    "\n",
    "df_gender = names_ratings.loc[names_ratings['rating.mean_gender'].notna(), ['name', 'rating.mean_gender', 'gender', 'name_type']]\n",
    "print(df_gender.head(), len(df_gender))\n",
    "\n",
    "df_polarity = names_ratings.loc[names_ratings['rating.mean_valence'].notna(), ['name', 'rating.mean_valence', 'polarity', 'name_type']]\n",
    "print(df_polarity.head(), len(df_polarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lkzcOAHFNsq7"
   },
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ubCvcOkNtgQ"
   },
   "outputs": [],
   "source": [
    "def fnn_maker(x_train, y_train, x_test, y_test, nodes, dropout):\n",
    "  # Input:\n",
    "  # - x_train = array of embeddings used to train the model\n",
    "  # - y_train = array of ratings used to train the model\n",
    "  # - x_test = array of embeddings used to test the model\n",
    "  # - y_test = array of ratings used to test the model\n",
    "  # - nodes = integer indicating the number of nodes to use in the hidden layer\n",
    "  # - dropout = integer indicating the amount of dropout to use in the hidden layer\n",
    "\n",
    "  # Process:\n",
    "  # Train a sequential NN using the train set and return the model & test MSE\n",
    "\n",
    "  # Output:\n",
    "  # - mse = test set mean squared error\n",
    "  # - fnn_model = trained neural network model\n",
    "\n",
    "  random.seed(17042020)                                                         # Set the seed using python's built-in seed function\n",
    "  set_seed(17042020)                                                            # Set the seed using keras/tensorflow's seed function, just to be sure\n",
    "\n",
    "  fnn_model = Sequential()                                                      # Initialize a sequential NN\n",
    "\n",
    "  fnn_model.add(Dense(nodes, input_dim=300, kernel_initializer=HeNormal(), \n",
    "                      activation=keras.layers.LeakyReLU()))                     # Add a dense layer with the specified nodes\n",
    "  fnn_model.add(Dropout(dropout))\n",
    "\n",
    "  fnn_model.add(Dense(1, activation='linear'))                                  # Add a final layer\n",
    "\n",
    "  callback = EarlyStopping(monitor = 'loss', patience=3, verbose=0)             # Add early stopping that stops after 3 rounds without improvement\n",
    "\n",
    "  fnn_model.compile(optimizer=Adam(), loss='mean_squared_error')                # Compile the model with mean squared loss\n",
    "\n",
    "  fnn_model.fit(x_train, y_train, epochs=100, batch_size=len(x_train), \n",
    "                callbacks=[callback], verbose=0)                                # Fit the model on the train set\n",
    "\n",
    "  return fnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cHA4BkeN_ur"
   },
   "outputs": [],
   "source": [
    "def model_evaluator(trained_model, x_train, x_test, y_train, y_test, pred_dict, \n",
    "                    test_names, test_name_types, lexical = None):\n",
    "  \n",
    "  #### Add predictions to the pred_dict\n",
    "  if lexical is True:\n",
    "    mean_vec_prediction = float(trained_model.predict(mean_vector_subwordless.reshape(1, -1), verbose=0))\n",
    "  else:\n",
    "    mean_vec_prediction = float(trained_model.predict(mean_vector.reshape(1, -1), verbose=0))\n",
    "\n",
    "  for n, t, i, j in zip(test_names, test_name_types, y_test, x_test):           # For every name in the test set\n",
    "      n = n.lower()                                                             # Convert name to lowercase\n",
    "      pred_dict[n] = [t, i, float(trained_model.predict(j.reshape(1, -1), verbose=0))]     # Add predictions to the predictions dictionary\n",
    "      pred_dict[n + '_mean_vector'] = [t, i, mean_vec_prediction]               # Add predictions to the mean_vector predictions dictionary\n",
    "\n",
    "  \n",
    "  ##############################################################################\n",
    "\n",
    "  y_pred = trained_model.predict(x_test, verbose=0)\n",
    "\n",
    "  mae_test = mean_absolute_error(y_test, y_pred)\n",
    "  \n",
    "  ##### MAE per name type ######################################################\n",
    "\n",
    "  type_dict = {}\n",
    "  type_counter = {}\n",
    "  for n, i, j in zip(test_name_types, y_test, x_test):                          # For every name type (i.e., real, talking, and madeup)\n",
    "    if n in type_dict.keys():\n",
    "      type_dict[n] = type_dict[n] + abs(i - trained_model.predict(j.reshape(1, -1)))   # Append the MAE for every name given that type (so that you get a sum of MAEs; one for each name)\n",
    "      type_counter[n] = type_counter[n] + 1                                     # And count the number of names given that type\n",
    "    else:\n",
    "      type_dict[n] = abs(i - trained_model.predict(j.reshape(1, -1), verbose=0))\n",
    "      type_counter[n] = 1\n",
    "\n",
    "  for i in type_dict.keys():\n",
    "    globals()[f\"mae_{i}\"] = float(type_dict[i])/float(type_counter[i])          # Calculate the average MAE per name type: (sum of MAEs for name type / name counter for name type)\n",
    "\n",
    "  if 'madeup' not in type_dict.keys():\n",
    "    globals()[f\"mae_madeup\"] = None\n",
    "\n",
    "  if 'talking' not in type_dict.keys():\n",
    "    globals()[f\"mae_talking\"] = None\n",
    "\n",
    "  if 'real' not in type_dict.keys():\n",
    "    globals()[f\"mae_real\"] = None\n",
    "\n",
    "  ##### Mean Only ##############################################################\n",
    "\n",
    "  if lexical is True:\n",
    "    mean_vec_array = np.full((len(x_test), 300), mean_vector_subwordless)\n",
    "  else:\n",
    "    mean_vec_array = np.full((len(x_test), 300), mean_vector)                     # Create a mean vector array with length = test_set_length, and width = 300\n",
    "\n",
    "  mean_vec_mae_test = mean_absolute_error(y_test, trained_model.predict(mean_vec_array, verbose=0))# Retrieve the MAE for the mean vector array\n",
    "\n",
    "  return mae_test, mae_madeup, mae_real, mae_talking, mean_vec_mae_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiPkTFDaOCCI"
   },
   "outputs": [],
   "source": [
    "column_list_metrics = ['dimension', 'analysis_type', 'mae_test', 'mae_madeup', \n",
    "                       'mae_real', 'mae_talking', 'mean_vec_mae_test', 'sd_mae_total', \n",
    "                       'sd_mae_madeup', 'sd_mae_real', 'sd_mae_talking', 'sd_mean_vec_mae']\n",
    "\n",
    "column_list_pred = ['Name', 'NameType', 'TrueRating', 'AnalysisType', 'Prediction', 'MeanVecPrediction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YqlZCe34OEB2"
   },
   "outputs": [],
   "source": [
    "def prediction_csv_maker(names_df, ngram, ngram_lex, lexical, file_name):\n",
    "  predictions_df = []                                                           # Create a list of lists that will be converted to a dataframe\n",
    "\n",
    "  for i in sorted(names_df['name']):                                            # For every name in the dataframe\n",
    "    i = i.lower()\n",
    "\n",
    "    predictions_df.append([i, ngram[i][0], ngram[i][1], 'Ngram', ngram[i][2],   # Append the name, name type, and normal + mean_vector only predictions for \n",
    "                          ngram[i + '_mean_vector'][2]])                        # the three model types as a row to the list of lists\n",
    "    \n",
    "    predictions_df.append([i, ngram[i][0], ngram[i][1], 'NgramLex',\n",
    "                          ngram_lex[i][2], ngram_lex[i + '_mean_vector'][2]])\n",
    "    \n",
    "    predictions_df.append([i, ngram[i][0], ngram[i][1], 'Lexical',\n",
    "                          lexical[i][2], lexical[i + '_mean_vector'][2]])\n",
    "    \n",
    "  predictions_df = pd.DataFrame(predictions_df, columns=column_list_pred)       # Convert list of lists to DF\n",
    "  predictions_df.to_csv(csv_path + file_name, index=False)                      # Save DF as .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rdz55UwQOFoW"
   },
   "outputs": [],
   "source": [
    "def fnn_maker_and_evaluator(x_train, y_train, x_test, y_test, pred_dict, \n",
    "                            test_names, test_name_types, nodes, dropout, analysis_type, \n",
    "                            dictionary, lexical = None):\n",
    "  # Input:\n",
    "  # - x_train = array of embeddings used to train the model\n",
    "  # - y_train = array of ratings used to train the model\n",
    "  # - x_test = array of embeddings used to test the model\n",
    "  # - y_test = array of ratings used to test the model\n",
    "  # - pred_dict = a dictionary that will be filled with predictions per name\n",
    "  # - test_names = dataframe containing the full names (i.e., not the embeddings) \n",
    "  # - test_name_types = dataframe containing the name type (real, madeup, talking)\n",
    "  # - nodes = integer indicating the number of nodes to use in the hidden layer\n",
    "  # - dropout = integer indicating the amount of dropout to use in the hidden layer\n",
    "\n",
    "  # Process:\n",
    "  # Train a sequential NN using the train set using fnn_maker(). Then, evaluate\n",
    "  # the model using model_evaluator and return the values.\n",
    "\n",
    "  # Output:\n",
    "  # Too many to explain here. Basically, a bunch of metrics to test the model.\n",
    "\n",
    "  #_, fnn_model = fnn_maker(x_train, y_train, x_test, y_test, nodes, dropout)\n",
    "  \n",
    "  fnn_model = fnn_maker(x_train, y_train, x_test, y_test, nodes, dropout)\n",
    "\n",
    "  mae_test, mae_madeup, mae_real, mae_talking, mean_vec_mae_test \\\n",
    "  = model_evaluator(fnn_model, x_train, x_test, y_train, y_test, \n",
    "                    pred_dict, test_names, test_name_types, lexical)\n",
    "  \n",
    "  dictionary[analysis_type].append([mae_test, mae_madeup, mae_real, mae_talking, mean_vec_mae_test])\n",
    "\n",
    "  return mae_test, mae_madeup, mae_real, mae_talking, mean_vec_mae_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ME7gCU3GOHqT"
   },
   "outputs": [],
   "source": [
    "def splitter(df, rating, train_index, test_index):\n",
    "  x_train_unfasttexted = df.iloc[train_index]                                 # Split the data into x_train and x_test\n",
    "  x_test_unfasttexted = df.iloc[test_index]                   \n",
    "  \n",
    "  x_train_ngram = fasttext_xifyer_ngram_v2(x_train_unfasttexted)                 # Get the word embeddings\n",
    "  x_train_ngram_lex = fasttext_xifyer_ngram_v2(x_train_unfasttexted, lexical = True)\n",
    "  x_train_lexical = fasttext_xifyer_lexical_v2(x_train_unfasttexted)\n",
    "\n",
    "  x_test_ngram = fasttext_xifyer_ngram_v2(x_test_unfasttexted)\n",
    "  x_test_ngram_lex = fasttext_xifyer_ngram_v2(x_test_unfasttexted, lexical = True)\n",
    "  x_test_lexical = fasttext_xifyer_lexical_v2(x_test_unfasttexted)\n",
    "\n",
    "  y_train = df.iloc[train_index][rating]                                      # Split the data into y_train and y_test\n",
    "  y_test = df.iloc[test_index][rating]\n",
    "\n",
    "  test_names = df.iloc[test_index]['name']                                    # Get a list of the names in the test set\n",
    "  test_name_types = df.iloc[test_index]['name_type']                          # Get a list of name types corresponding to the names in the test set\n",
    "\n",
    "  return x_train_ngram, x_test_ngram, x_train_ngram_lex, x_test_ngram_lex, x_train_lexical, \\\n",
    "         x_test_lexical, y_train, y_test, test_names, test_name_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eAnmzeo2OJYi"
   },
   "outputs": [],
   "source": [
    "def sd_calculator(dictionary, type_list):\n",
    "  sd_dict_nn = {'ngram' : {'total' : [], 'madeup': [], 'real': [], 'talking': [], 'mean_vec_mae': []},\n",
    "                'ngram_lex' : {'total' : [], 'madeup': [], 'real': [], 'talking': [], 'mean_vec_mae': []}, \n",
    "                'lexical': {'total' : [], 'madeup': [], 'real': [], 'talking': [], 'mean_vec_mae': []}}\n",
    "\n",
    "  for analysis_type in type_list:    \n",
    "    sd_mae_total = []\n",
    "    sd_mae_madeup = []\n",
    "    sd_mae_real = []\n",
    "    sd_mae_talking = []\n",
    "\n",
    "    sd_mean_vec_mae = []\n",
    "\n",
    "    for iteration in dictionary[analysis_type]:\n",
    "      sd_mae_total.append(iteration[0])\n",
    "      sd_mae_madeup.append(iteration[1])\n",
    "      sd_mae_real.append(iteration[2])\n",
    "      sd_mae_talking.append(iteration[3])\n",
    "\n",
    "      sd_mean_vec_mae.append(iteration[4])\n",
    "\n",
    "    sd_dict_nn[analysis_type]['total'] = np.std(sd_mae_total)\n",
    "    if analysis_type == 'ngram':\n",
    "      sd_mae_madeup = list(filter(None, sd_mae_madeup))\n",
    "      sd_dict_nn[analysis_type]['madeup'] = np.std(sd_mae_madeup)\n",
    "    else:\n",
    "      sd_dict_nn[analysis_type]['madeup'] = None\n",
    "    \n",
    "    sd_mae_real = list(filter(None, sd_mae_real))\n",
    "    sd_dict_nn[analysis_type]['real'] = np.std(sd_mae_real)\n",
    "\n",
    "    sd_mae_talking = list(filter(None, sd_mae_talking))\n",
    "    sd_dict_nn[analysis_type]['talking'] = np.std(sd_mae_talking)\n",
    "\n",
    "    sd_mae_sd_mean_vec_mae = list(filter(None, sd_mean_vec_mae))\n",
    "    sd_dict_nn[analysis_type]['mean_vec_mae'] = np.std(sd_mean_vec_mae)\n",
    "\n",
    "  return sd_dict_nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSofECb_OLnv"
   },
   "outputs": [],
   "source": [
    "def list_creator(dictionary):\n",
    "  ngram_list = dictionary['ngram']\n",
    "  ngram_lex_list = dictionary['ngram_lex']\n",
    "  lexical_list = dictionary['lexical']                        \n",
    "\n",
    "  ngram_list = [np.mean(x) for x in [list(filter(None, x)) for x in zip(*ngram_list)]]\n",
    "  ngram_lex_list = [np.mean(x) for x in [list(filter(None, x)) for x in zip(*ngram_lex_list)]]                                                  \n",
    "  lexical_list = [np.mean(x) for x in [list(filter(None, x)) for x in zip(*lexical_list)]] \n",
    "\n",
    "  return ngram_list, ngram_lex_list, lexical_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aFBOu1adONtI"
   },
   "outputs": [],
   "source": [
    "def metrics_csv_maker(sd_dict_nn, ngram_list, ngram_lex_list, \n",
    "                      lexical_list, type_list, dimension):\n",
    "  csv_df = []                                                                   # Create a list of lists that will be converted to a dataframe\n",
    "\n",
    "  for value_list, analysis_type in zip([ngram_list, ngram_lex_list,             # Given the list of metrics for every model (combined, ngram, lexical)\n",
    "                                        lexical_list], type_list):\n",
    "    value_list.insert(0, analysis_type)                                         # Insert the name of the analysis type to the values (i.e., 'combined', etc.)\n",
    "    value_list.insert(0, dimension)                                             # Insert the name of the dimension (i.e., 'age', 'gender', 'polarity') to the values\n",
    "    value_list.append(sd_dict_nn[analysis_type]['total'])\n",
    "    value_list.append(sd_dict_nn[analysis_type]['madeup'])\n",
    "    value_list.append(sd_dict_nn[analysis_type]['real'])\n",
    "    value_list.append(sd_dict_nn[analysis_type]['talking'])\n",
    "    value_list.append(sd_dict_nn[analysis_type]['mean_vec_mae'])\n",
    "    csv_df.append(value_list)                                                   # Add the list of values as a row to the DF list of lists\n",
    "  \n",
    "  csv_df = pd.DataFrame(csv_df, columns=column_list_metrics)                    # Convert the list of lists to a DF\n",
    "  csv_df.to_csv(csv_path + dimension +'_nn_metrics.csv', index=False)           # Save the DF as a .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JPVE4VZwOPW7"
   },
   "outputs": [],
   "source": [
    "def neural_network_k_folder(df, rating, dimension, dictionary, nodes, dropout):\n",
    "  # Input:\n",
    "  # - df = a dataframe with the name, name_type, and rating for the dimension at  \n",
    "  # hand (i.e., age, gender, or polarity)\n",
    "  # - rating = a string indicating what rating to extract from the df\n",
    "  # - dimension = a string indicating what dimension is considered (i.e., 'age',\n",
    "  # 'gender', or 'polarity')\n",
    "  # - dictionary = an empty dictionary to store the MSE output by fnn_maker() in \n",
    "  # per configuration\n",
    "  # - nodes = integer indicating the number of nodes to use in the hidden layer\n",
    "  # - dropout = integer indicating the amount of dropout to use in the hidden layer\n",
    "\n",
    "  # Process:\n",
    "  # Given the df, get 5 train/test splits, and per fold, train a NN model using\n",
    "  # fnn_maker() for the combined, ngram, and lexical data. Then, store these\n",
    "  # metrics in a .csv file.\n",
    "\n",
    "  # Output: \n",
    "  # - pred_dict_combined: y_true and y_pred (for several conditions) per name (combined)\n",
    "  # - pred_dict_ngram: y_true and y_pred (for several conditions) per name (ngram)\n",
    "  # - pred_dict_lexical: y_true and y_pred (for several conditions) per name (lexical)\n",
    "\n",
    "  type_list = ['ngram', 'ngram_lex', 'lexical']                                 # List indicating the model type\n",
    "\n",
    "  loocv = LeaveOneOut()                                                         # Set up stratified LOOCV\n",
    "  \n",
    "  pred_dict_ngram = {}                                                          # Set up two dictionaries to store the predicted y-values for the test names in\n",
    "  pred_dict_ngram_lex = {}\n",
    "  pred_dict_lexical = {}\n",
    "\n",
    "  for train_index, test_index in loocv.split(df):                               # For every fold (stratified on the name type, i.e., real, madeup, or talking)\n",
    "    x_train_ngram, x_test_ngram, x_train_ngram_lex, x_test_ngram_lex, x_train_lexical, \\\n",
    "    x_test_lexical, y_train, y_test, test_names, test_name_types = splitter(df, rating, train_index, test_index)                      \n",
    "\n",
    "    fnn_maker_and_evaluator(x_train_ngram, y_train, x_test_ngram, y_test, pred_dict_ngram,\n",
    "                            test_names, test_name_types, nodes, dropout, 'ngram', dictionary) # Train the NN given the fold, and return all of the variables of interest (ngram)\n",
    "  \n",
    "    fnn_maker_and_evaluator(x_train_ngram_lex, y_train, x_test_ngram_lex, y_test, \n",
    "                            pred_dict_ngram_lex, test_names, test_name_types, nodes,\n",
    "                            dropout, 'ngram_lex', dictionary)                        # Train the NN given the fold, and return all of the variables of interest (ngram)\n",
    "\n",
    "    fnn_maker_and_evaluator(x_train_lexical, y_train, x_test_lexical, y_test, \n",
    "                            pred_dict_lexical, test_names, test_name_types, nodes,\n",
    "                            dropout, 'lexical', dictionary, lexical = True)         # Train the NN given the fold, and return all of the variables of interest (lexical)\n",
    "\n",
    "  sd_dict_nn = sd_calculator(dictionary, type_list)                             # calculate standard deviations\n",
    "\n",
    "  ngram_list, ngram_lex_list, lexical_list = list_creator(dictionary)           # create list of metrics for all analysis types\n",
    "\n",
    "  metrics_csv_maker(sd_dict_nn, ngram_list, ngram_lex_list, lexical_list, type_list, dimension)     # Call function that creates CSV output file for the metrics          \n",
    "\n",
    "  return pred_dict_ngram, pred_dict_ngram_lex, pred_dict_lexical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PMnXvJzDOSRo"
   },
   "source": [
    "## Running the Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S4FgZTjbOQ3w"
   },
   "outputs": [],
   "source": [
    "age_dict_nn_final = {'ngram' : [], 'ngram_lex' : [], 'lexical': []}          # initialize the score dictionary for age\n",
    "\n",
    "age_pred_dict_ngram_nn, age_pred_dict_ngram_lex_nn, \\\n",
    "age_pred_dict_lexical_nn = \\\n",
    "neural_network_k_folder(df_age, 'rating.mean_age', 'age', \n",
    "                        age_dict_nn_final, 256, 0.5)                            # Perform the 5-fold cross validation and save the metrics as a .csv file\n",
    "\n",
    "prediction_csv_maker(df_age, age_pred_dict_ngram_nn, age_pred_dict_ngram_lex_nn, \n",
    "                     age_pred_dict_lexical_nn, 'age_pred_nn.csv')               # Save predictions per name and model type to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwZLT_euOWVB"
   },
   "outputs": [],
   "source": [
    "gender_dict_nn_final = {'ngram' : [], 'ngram_lex' : [], 'lexical': []}          # initialize the score dictionary for gender\n",
    "\n",
    "gender_pred_dict_ngram_nn, gender_pred_dict_ngram_lex_nn, \\\n",
    "gender_pred_dict_lexical_nn = \\\n",
    "neural_network_k_folder(df_gender, 'rating.mean_gender', 'gender', \n",
    "                        gender_dict_nn_final, 512, 0.2)                         # Perform the 5-fold cross validation and save the metrics as a .csv file\n",
    "\n",
    "prediction_csv_maker(df_gender, gender_pred_dict_ngram_nn, gender_pred_dict_ngram_lex_nn, \n",
    "                     gender_pred_dict_lexical_nn, 'gender_pred_nn.csv')         # Save predictions per name and model type to a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nZOExvyoOXnq"
   },
   "outputs": [],
   "source": [
    "polarity_dict_nn_final = {'ngram' : [], 'ngram_lex' : [], 'lexical': []}        # initialize the score dictionary for polarity\n",
    "\n",
    "polarity_pred_dict_ngram_nn, polarity_pred_dict_ngram_lex_nn, \\\n",
    "polarity_pred_dict_lexical_nn = \\\n",
    "neural_network_k_folder(df_polarity, 'rating.mean_valence', 'polarity', \n",
    "                        polarity_dict_nn_final, 512, 0.5)                       # Perform the 5-fold cross validation and save the metrics as a .csv file\n",
    "\n",
    "prediction_csv_maker(df_polarity, polarity_pred_dict_ngram_nn, polarity_pred_dict_ngram_lex_nn, \n",
    "                     polarity_pred_dict_lexical_nn, 'polarity_pred_nn.csv')     # Save predictions per name and model type to a csv"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOCYznvqxLgvCWbGNiEcPa4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
